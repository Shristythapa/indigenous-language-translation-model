{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyOYr2Y5rgJ7/+NxwiRBTfjW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shristythapa/indigenous-language-translation-model/blob/main/language_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "1x-eALY9AMzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEpMsE1J_lQK"
      },
      "outputs": [],
      "source": [
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import math\n",
        "import psutil\n",
        "import time\n",
        "import datetime\n",
        "from io import open\n",
        "import random\n",
        "from random import shuffle\n",
        "import argparse\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import torch.cuda\n",
        "\n",
        "\"\"\"this line clears sys to allow for argparse to work as gradient clipper\"\"\"\n",
        "import sys; sys.argv=['']; del sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "print(use_cuda)"
      ],
      "metadata": {
        "id": "_oRAuuOKAWUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"This function converts a Unicode string to plain ASCII\n",
        "from https://stackoverflow.com/a/518232/2809427\"\"\"\n",
        "def uniToAscii(sentence):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', sentence)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "\n",
        "\"\"\"Lowercase, trim, and remove non-letter characters (from pytorch)\"\"\"\n",
        "def normalizeString(s):\n",
        "    s = re.sub(r\" ##AT##-##AT## \", r\" \", s)\n",
        "    s = uniToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "\"\"\"Denote patterns that sentences must start with to be kept in dataset.\n",
        "Can be changed if desired (from pytorch)\"\"\"\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s\",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"Filters each input-output pair, keeping sentences that are less than max_length\n",
        "if start_filter is true, also filters out sentences that don't start with eng_prefixes\"\"\"\n",
        "def filterPair(p, max_length, start_filter):\n",
        "    filtered = len(p[0].split(' ')) < max_length and \\\n",
        "        len(p[1].split(' ')) < max_length\n",
        "    if start_filter:\n",
        "        return filtered and p[1].startswith(eng_prefixes)\n",
        "    else:\n",
        "        return filtered\n",
        "\n",
        "\"\"\"Filters all of the input-output language pairs in the dataset using filterPair\n",
        "for each pair (from pytorch)\"\"\"\n",
        "def filterPairs(pairs, max_length, start_filter):\n",
        "    return [pair for pair in pairs if filterPair(pair, max_length, start_filter)]"
      ],
      "metadata": {
        "id": "Ti8qPS9BZJRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Three special tokens are defined with specific index values: SOS_token (start of sentence), EOS_token (end of sentence),\n",
        " and UNK_token (unknown word). These tokens are used to handle special cases in language processing\n",
        "where standard language processing rules or procudueres may not apply\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"start of sentence tag\"\"\"\n",
        "SOS_token = 0\n",
        "\n",
        "\"\"\"end of sentence tag\"\"\"\n",
        "EOS_token = 1\n",
        "\n",
        "\"\"\"unknown word tag (this is used to handle words that are not in our Vocabulary)\"\"\"\n",
        "UNK_token = 2\n",
        "\n",
        "\n",
        "\"\"\"Lang class, used to store the vocabulary of each language\"\"\"\n",
        "class Lang:\n",
        "    def __init__(self, language):\n",
        "        self.language_name = language\n",
        "        self.word_to_index = {\"SOS\":SOS_token, \"EOS\":EOS_token, \"<UNK>\":UNK_token}\n",
        "        self.word_to_count = {}\n",
        "        self.index_to_word = {SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token: \"<UNK>\"}\n",
        "        self.vocab_size = 3\n",
        "        self.cutoff_point = -1\n",
        "\n",
        "\n",
        "    def countSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.countWords(word)\n",
        "\n",
        "    \"\"\"counts the number of times each word appears in the dataset\"\"\"\n",
        "    def countWords(self, word):\n",
        "        if word not in self.word_to_count:\n",
        "            self.word_to_count[word] = 1\n",
        "        else:\n",
        "            self.word_to_count[word] += 1\n",
        "\n",
        "    \"\"\"if the number of unique words in the dataset is larger than the\n",
        "    specified max_vocab_size, creates a cutoff point that is used to\n",
        "    leave infrequent words out of the vocabulary\"\"\"\n",
        "    def createCutoff(self, max_vocab_size):\n",
        "        word_freqs = list(self.word_to_count.values())\n",
        "        word_freqs.sort(reverse=True)\n",
        "        if len(word_freqs) > max_vocab_size:\n",
        "            self.cutoff_point = word_freqs[max_vocab_size]\n",
        "\n",
        "    \"\"\"assigns each unique word in a sentence a unique index\"\"\"\n",
        "    def addSentence(self, sentence):\n",
        "        new_sentence = ''\n",
        "        for word in sentence.split(' '):\n",
        "            unk_word = self.addWord(word)\n",
        "            if not new_sentence:\n",
        "                new_sentence =unk_word\n",
        "            else:\n",
        "                new_sentence = new_sentence + ' ' + unk_word\n",
        "        return new_sentence\n",
        "\n",
        "    \"\"\"assigns a word a unique index if not already in vocabulary\n",
        "    and it appeaars often enough in the dataset\n",
        "    (self.word_to_count is larger than self.cutoff_point)\"\"\"\n",
        "    def addWord(self, word):\n",
        "        if self.word_to_count[word] > self.cutoff_point:\n",
        "            if word not in self.word_to_index:\n",
        "                self.word_to_index[word] = self.vocab_size\n",
        "                self.index_to_word[self.vocab_size] = word\n",
        "                self.vocab_size += 1\n",
        "            return word\n",
        "        else:\n",
        "            return self.index_to_word[2]"
      ],
      "metadata": {
        "id": "lugegIM3ZJnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''prepares both the input and output Lang classes from the passed dataset'''\n",
        "\n",
        "def prepareLangs(lang1, lang2, file_path, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    if len(file_path) == 2:\n",
        "        lang1_lines = open(file_path[0], encoding='utf-8').\\\n",
        "            read().strip().split('\\n')\n",
        "\n",
        "        lang2_lines = open(file_path[1], encoding='utf-8').\\\n",
        "            read().strip().split('\\n')\n",
        "\n",
        "        if len(lang1_lines) != len(lang2_lines):\n",
        "            print(\"Input and output text sizes do not align\")\n",
        "            print(\"Number of lang1 lines: %s \" %len(lang1_lines))\n",
        "            print(\"Number of lang2 lines: %s \" %len(lang2_lines))\n",
        "            quit()\n",
        "\n",
        "        pairs = []\n",
        "\n",
        "        for line in range(len(lang1_lines)):\n",
        "            pairs.append([normalizeString(lang1_lines[line]),\n",
        "                          normalizeString(lang2_lines[line])])\n",
        "\n",
        "\n",
        "    elif len(file_path) == 1:\n",
        "        lines = open(file_path[0], encoding='utf-8').\\\n",
        "    \tread().strip().split('\\n')\n",
        "        pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "def prepareLangs(lang1, lang2, file_path, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    if len(file_path) == 2:\n",
        "        lang1_lines = open(file_path[0], encoding='utf-8').\\\n",
        "            read().strip().split('\\n')\n",
        "\n",
        "        lang2_lines = open(file_path[1], encoding='utf-8').\\\n",
        "            read().strip().split('\\n')\n",
        "\n",
        "        if len(lang1_lines) != len(lang2_lines):\n",
        "            print(\"Input and output text sizes do not align\")\n",
        "            print(\"Number of lang1 lines: %s \" %len(lang1_lines))\n",
        "            print(\"Number of lang2 lines: %s \" %len(lang2_lines))\n",
        "            quit()\n",
        "\n",
        "        pairs = []\n",
        "\n",
        "        for line in range(len(lang1_lines)):\n",
        "            pairs.append([normalizeString(lang1_lines[line]),\n",
        "                          normalizeString(lang2_lines[line])])\n",
        "\n",
        "\n",
        "    elif len(file_path) == 1:\n",
        "        lines = open(file_path[0], encoding='utf-8').\\\n",
        "    \tread().strip().split('\\n')\n",
        "        pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "metadata": {
        "id": "OlEcnw5pvHm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"completely prepares both input and output languages\n",
        "and returns cleaned and trimmed train and test pairs\"\"\"\n",
        "\n",
        "def prepareData(lang1, lang2, file_path, max_vocab_size=50000,\n",
        "                reverse=False, trim=0, start_filter=False, perc_train_set=0.9,\n",
        "                print_to=None):\n",
        "\n",
        "    input_lang, output_lang, pairs = prepareLangs(lang1, lang2,\n",
        "                                                  file_path, reverse)\n",
        "\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "\n",
        "    if print_to:\n",
        "        with open(print_to,'a') as f:\n",
        "            f.write(\"Read %s sentence pairs \\n\" % len(pairs))\n",
        "\n",
        "    if trim != 0:\n",
        "        pairs = filterPairs(pairs, trim, start_filter)\n",
        "        print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "        if print_to:\n",
        "            with open(print_to,'a') as f:\n",
        "                f.write(\"Read %s sentence pairs \\n\" % len(pairs))\n",
        "\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.countSentence(pair[0])\n",
        "        output_lang.countSentence(pair[1])\n",
        "\n",
        "\n",
        "    input_lang.createCutoff(max_vocab_size)\n",
        "    output_lang.createCutoff(max_vocab_size)\n",
        "\n",
        "    pairs = [(input_lang.addSentence(pair[0]),output_lang.addSentence(pair[1]))\n",
        "             for pair in pairs]\n",
        "\n",
        "    shuffle(pairs)\n",
        "\n",
        "    train_pairs = pairs[:math.ceil(perc_train_set*len(pairs))]\n",
        "    test_pairs = pairs[math.ceil(perc_train_set*len(pairs)):]\n",
        "\n",
        "    print(\"Train pairs: %s\" % (len(train_pairs)))\n",
        "    print(\"Test pairs: %s\" % (len(test_pairs)))\n",
        "    print(\"Counted Words -> Trimmed Vocabulary Sizes (w/ EOS and SOS tags):\")\n",
        "    print(\"%s, %s -> %s\" % (input_lang.language_name, len(input_lang.word_to_count),\n",
        "                            input_lang.vocab_size,))\n",
        "    print(\"%s, %s -> %s\" % (output_lang.language_name, len(output_lang.word_to_count),\n",
        "                            output_lang.vocab_size))\n",
        "    print()\n",
        "\n",
        "    if print_to:\n",
        "        with open(print_to,'a') as f:\n",
        "            f.write(\"Train pairs: %s\" % (len(train_pairs)))\n",
        "            f.write(\"Test pairs: %s\" % (len(test_pairs)))\n",
        "            f.write(\"Counted Words -> Trimmed Vocabulary Sizes (w/ EOS and SOS tags):\")\n",
        "            f.write(\"%s, %s -> %s\" % (input_lang.language_name,\n",
        "                                      len(input_lang.word_to_count),\n",
        "                                      input_lang.vocab_size,))\n",
        "            f.write(\"%s, %s -> %s \\n\" % (output_lang.language_name, len(output_lang.word_to_count),\n",
        "                            output_lang.vocab_size))\n",
        "\n",
        "    return input_lang, output_lang, train_pairs, test_pairs"
      ],
      "metadata": {
        "id": "SKjkXoQRwyWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"converts a sentence to one hot encoding vectors - pytorch allows us to just\n",
        "use the number corresponding to the unique index for that word,\n",
        "rather than a complete one hot encoding vector for each word\"\"\"\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    indexes = []\n",
        "    for word in sentence.split(' '):\n",
        "        try:\n",
        "            indexes.append(lang.word_to_index[word])\n",
        "        except:\n",
        "            indexes.append(lang.word_to_index[\"<UNK>\"])\n",
        "    return indexes\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    result = torch.LongTensor(indexes).view(-1)\n",
        "    if use_cuda:\n",
        "        return result.cuda()\n",
        "    else:\n",
        "        return result\n",
        "\n",
        "\"\"\"converts a pair of sentence (input and target) to a pair of tensors\"\"\"\n",
        "def tensorsFromPair(input_lang, output_lang, pair):\n",
        "    input_variable = tensorFromSentence(input_lang, pair[0])\n",
        "    target_variable = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_variable, target_variable)\n",
        "\n",
        "\n",
        "\"\"\"converts from tensor of one hot encoding vector indices to sentence\"\"\"\n",
        "def sentenceFromTensor(lang, tensor):\n",
        "    raw = tensor.data\n",
        "    words = []\n",
        "    for num in raw:\n",
        "        words.append(lang.index_to_word[num.item()])\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "5q7QjaJGw5gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"seperates data into batches of size batch_size\"\"\"\n",
        "def batchify(data, input_lang, output_lang, batch_size, shuffle_data=True):\n",
        "    if shuffle_data == True:\n",
        "        shuffle(data)  # Shuffling the data if requested.\n",
        "\n",
        "    number_of_batches = len(data) // batch_size  # Calculate the number of batches.\n",
        "    batches = list(range(number_of_batches))  # Initialize a list to store batches.\n",
        "    longest_elements = list(range(number_of_batches))  # Initialize a list to store the length of longest elements in each batch.\n",
        "\n",
        "    for batch_number in range(number_of_batches):\n",
        "        longest_input = 0  # Initialize variables to keep track of the longest input and target sequences in this batch.\n",
        "        longest_target = 0\n",
        "        input_variables = list(range(batch_size))  # Initialize lists to store input and target variables for this batch.\n",
        "        target_variables = list(range(batch_size))\n",
        "        index = 0  # Initialize an index variable to keep track of the position in the batch.\n",
        "\n",
        "        # Iterate over the data to create this batch.\n",
        "        for pair in range((batch_number * batch_size), ((batch_number + 1) * batch_size)):\n",
        "            input_variables[index], target_variables[index] = tensorsFromPair(input_lang, output_lang, data[pair])\n",
        "\n",
        "            # Update the longest input and target lengths if necessary.\n",
        "            if len(input_variables[index]) >= longest_input:\n",
        "                longest_input = len(input_variables[index])\n",
        "            if len(target_variables[index]) >= longest_target:\n",
        "                longest_target = len(target_variables[index])\n",
        "\n",
        "            index += 1  # Move to the next position in the batch.\n",
        "\n",
        "        # Store the input and target variables for this batch, along with their longest lengths.\n",
        "        batches[batch_number] = (input_variables, target_variables)\n",
        "        longest_elements[batch_number] = (longest_input, longest_target)\n",
        "\n",
        "    return batches, longest_elements, number_of_batches\n",
        "\n",
        "\n",
        "\n",
        "def pad_batch(batch):\n",
        "    padded_inputs = torch.nn.utils.rnn.pad_sequence(batch[0], padding_value=EOS_token)  # Pad the input sequences.\n",
        "    padded_targets = torch.nn.utils.rnn.pad_sequence(batch[1], padding_value=EOS_token)  # Pad the target sequences.\n",
        "    return (padded_inputs, padded_targets)  # Return the padded batch.\n"
      ],
      "metadata": {
        "id": "ILzqG1D2w-1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, layers=1, dropout=0.1,\n",
        "                 bidirectional=True):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "\n",
        "        # Determine the number of directions (1 for unidirectional, 2 for bidirectional).\n",
        "        if bidirectional:\n",
        "            self.directions = 2\n",
        "        else:\n",
        "            self.directions = 1\n",
        "\n",
        "        # Store the provided parameters as instance variables.\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Create an embedding layer to convert input tokens into continuous representations.\n",
        "        self.embedder = nn.Embedding(input_size, hidden_size)\n",
        "\n",
        "        # Apply dropout to the embeddings to prevent overfitting.\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Create an LSTM layer with specified parameters.\n",
        "        self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size,\n",
        "                            num_layers=layers, dropout=dropout,\n",
        "                            bidirectional=bidirectional, batch_first=False)\n",
        "\n",
        "        # Create a fully connected (linear) layer to transform LSTM outputs.\n",
        "        self.fc = nn.Linear(hidden_size * self.directions, hidden_size)\n",
        "\n",
        "    def forward(self, input_data, h_hidden, c_hidden):\n",
        "        # Embed the input data (token sequences).\n",
        "        embedded_data = self.embedder(input_data)\n",
        "\n",
        "        # Apply dropout to the embedded data.\n",
        "        embedded_data = self.dropout(embedded_data)\n",
        "\n",
        "        # Pass the embedded data through the LSTM layer.\n",
        "        hiddens, outputs = self.lstm(embedded_data, (h_hidden, c_hidden))\n",
        "\n",
        "        return hiddens, outputs\n",
        "\n",
        "    def create_init_hiddens(self, batch_size):\n",
        "        # Create initial hidden states for the encoder LSTM.\n",
        "        h_hidden = Variable(torch.zeros(self.num_layers * self.directions,\n",
        "                                        batch_size, self.hidden_size))\n",
        "        c_hidden = Variable(torch.zeros(self.num_layers * self.directions,\n",
        "                                        batch_size, self.hidden_size))\n",
        "\n",
        "        # Check if a GPU (CUDA) is available and move the tensors accordingly.\n",
        "        if torch.cuda.is_available():\n",
        "            return h_hidden.cuda(), c_hidden.cuda()\n",
        "        else:\n",
        "            return h_hidden, c_hidden\n"
      ],
      "metadata": {
        "id": "ZUx1g45axAfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderAttn(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, layers=1, dropout=0.1, bidirectional=True):\n",
        "        super(DecoderAttn, self).__init__()\n",
        "\n",
        "        # Determine the number of directions (1 for unidirectional, 2 for bidirectional).\n",
        "        if bidirectional:\n",
        "            self.directions = 2\n",
        "        else:\n",
        "            self.directions = 1\n",
        "\n",
        "        # Store the provided parameters as instance variables.\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Create an embedding layer for the decoder's output.\n",
        "        self.embedder = nn.Embedding(output_size, hidden_size)\n",
        "\n",
        "        # Apply dropout to the embeddings to prevent overfitting.\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Create a linear layer to learn attention scores.\n",
        "        self.score_learner = nn.Linear(hidden_size * self.directions,\n",
        "                                       hidden_size * self.directions)\n",
        "\n",
        "        # Create an LSTM layer with specified parameters.\n",
        "        self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size,\n",
        "                            num_layers=layers, dropout=dropout,\n",
        "                            bidirectional=bidirectional, batch_first=False)\n",
        "\n",
        "        # Create a linear layer to combine context and top hidden state.\n",
        "        self.context_combiner = nn.Linear((hidden_size * self.directions)\n",
        "                                          + (hidden_size * self.directions), hidden_size)\n",
        "\n",
        "        # Create a tanh activation function.\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        # Create an output linear layer.\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        # Create softmax and log-softmax activation functions for output probabilities.\n",
        "        self.soft = nn.Softmax(dim=1)\n",
        "        self.log_soft = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input_data, h_hidden, c_hidden, encoder_hiddens):\n",
        "        # Embed the input data (target sequences).\n",
        "        embedded_data = self.embedder(input_data)\n",
        "\n",
        "        # Apply dropout to the embedded data.\n",
        "        embedded_data = self.dropout(embedded_data)\n",
        "\n",
        "        # Get the batch size from the embedded data shape.\n",
        "        batch_size = embedded_data.shape[1]\n",
        "\n",
        "        # Pass the embedded data through the LSTM layer.\n",
        "        hiddens, outputs = self.lstm(embedded_data, (h_hidden, c_hidden))\n",
        "\n",
        "        # Extract the top hidden state from LSTM outputs.\n",
        "        top_hidden = outputs[0].view(self.num_layers, self.directions,\n",
        "                                     hiddens.shape[1], self.hidden_size)[self.num_layers-1]\n",
        "        top_hidden = top_hidden.permute(1, 2, 0).contiguous().view(batch_size, -1, 1)\n",
        "\n",
        "        # Learn attention scores.\n",
        "        prep_scores = self.score_learner(encoder_hiddens.permute(1, 0, 2))\n",
        "        scores = torch.bmm(prep_scores, top_hidden)\n",
        "\n",
        "        # Apply softmax to calculate attention scores.\n",
        "        attn_scores = self.soft(scores)\n",
        "\n",
        "        # Calculate the context matrix.\n",
        "        con_mat = torch.bmm(encoder_hiddens.permute(1, 2, 0), attn_scores)\n",
        "\n",
        "        # Combine context and top hidden state using tanh activation.\n",
        "        h_tilde = self.tanh(self.context_combiner(torch.cat((con_mat, top_hidden), dim=1).view(batch_size, -1)))\n",
        "\n",
        "        # Generate predictions using the combined context.\n",
        "        pred = self.output(h_tilde)\n",
        "\n",
        "        # Apply log-softmax to the predictions for output probabilities.\n",
        "        pred = self.log_soft(pred)\n",
        "\n",
        "        return pred, outputs\n"
      ],
      "metadata": {
        "id": "UzJ5m1hHxEFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Performs training on a single batch of training data. Computing the loss\n",
        "according to the passed loss_criterion and back-propagating on this loss.'''\n",
        "\n",
        "def train_batch(input_batch, target_batch, encoder, decoder,\n",
        "                encoder_optimizer, decoder_optimizer, loss_criterion):\n",
        "    # Reset the gradients of both the encoder and decoder.\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "    loss = 0  # Initialize the loss to zero.\n",
        "\n",
        "    # Create initial hidden states for the encoder based on the batch size.\n",
        "    enc_h_hidden, enc_c_hidden = encoder.create_init_hiddens(input_batch.shape[1])\n",
        "\n",
        "    # Pass the input batch through the encoder to obtain encoder hidden states.\n",
        "    enc_hiddens, enc_outputs = encoder(input_batch, enc_h_hidden, enc_c_hidden)\n",
        "\n",
        "    # Initialize the decoder input with the SOS (Start-of-Sequence) token.\n",
        "    # This is the first input token for the decoder.\n",
        "    decoder_input = Variable(torch.LongTensor(1, input_batch.shape[1]).fill_(output_lang.word_to_index.get(\"SOS\")).cuda()) if use_cuda \\\n",
        "                    else Variable(torch.LongTensor(1, input_batch.shape[1]).fill_(output_lang.word_to_index.get(\"SOS\")))\n",
        "\n",
        "    # Initialize the decoder hidden states with the encoder outputs.\n",
        "    dec_h_hidden = enc_outputs[0]\n",
        "    dec_c_hidden = enc_outputs[1]\n",
        "\n",
        "    for i in range(target_batch.shape[0]):\n",
        "        # Forward pass through the decoder for the current timestep.\n",
        "        pred, dec_outputs = decoder(decoder_input, dec_h_hidden, dec_c_hidden, enc_hiddens)\n",
        "\n",
        "        # Set the decoder input for the next timestep to the target sequence's current element.\n",
        "        decoder_input = target_batch[i].view(1, -1)\n",
        "\n",
        "        # Update decoder hidden states with the decoder's current outputs.\n",
        "        dec_h_hidden = dec_outputs[0]\n",
        "        dec_c_hidden = dec_outputs[1]\n",
        "\n",
        "        # Calculate the loss for the current prediction and target.\n",
        "        loss += loss_criterion(pred, target_batch[i])\n",
        "\n",
        "    # Backpropagate the gradients to compute parameter updates.\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients to prevent exploding gradients, a common technique in training.\n",
        "    torch.nn.utils.clip_grad_norm_(encoder.parameters(), args.clip)\n",
        "    torch.nn.utils.clip_grad_norm_(decoder.parameters(), args.clip)\n",
        "\n",
        "    # Update the encoder and decoder parameters using the optimizers.\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    # Calculate and return the average loss per target sequence element.\n",
        "    # The loss is divided by the number of target sequence elements to obtain the average.\n",
        "    return loss.item() / target_batch.shape[0]\n"
      ],
      "metadata": {
        "id": "YqqZDpSNxHPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Performs a complete epoch of training through all of the training_batches'''\n",
        "\n",
        "def train(train_batches, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_criterion):\n",
        "\n",
        "\tround_loss = 0\n",
        "\ti = 1\n",
        "\tfor batch in train_batches:\n",
        "\t\ti += 1\n",
        "\t\t(input_batch, target_batch) = pad_batch(batch)\n",
        "\t\tbatch_loss = train_batch(input_batch, target_batch, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_criterion)\n",
        "\t\tround_loss += batch_loss\n",
        "\n",
        "\treturn round_loss / len(train_batches)"
      ],
      "metadata": {
        "id": "lU1S4FY4xQv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Evaluates the loss on a single batch of test data. Computing the loss\n",
        "according to the passed loss_criterion. Does not perform back-prop'''\n",
        "\n",
        "def test_batch(input_batch, target_batch, encoder, decoder, loss_criterion):\n",
        "\n",
        "\tloss = 0\n",
        "\n",
        "\t#create initial hidde state for encoder\n",
        "\tenc_h_hidden, enc_c_hidden = encoder.create_init_hiddens(input_batch.shape[1])\n",
        "\n",
        "\tenc_hiddens, enc_outputs = encoder(input_batch, enc_h_hidden, enc_c_hidden)\n",
        "\n",
        "\tdecoder_input = Variable(torch.LongTensor(1,input_batch.shape[1]).\n",
        "                           fill_(output_lang.word_to_index.get(\"SOS\")).cuda()) if use_cuda \\\n",
        "\t\t\t\t\telse Variable(torch.LongTensor(1,input_batch.shape[1]).\n",
        "                        fill_(output_lang.word_to_index.get(\"SOS\")))\n",
        "\tdec_h_hidden = enc_outputs[0]\n",
        "\tdec_c_hidden = enc_outputs[1]\n",
        "\n",
        "\tfor i in range(target_batch.shape[0]):\n",
        "\t\tpred, dec_outputs = decoder(decoder_input, dec_h_hidden, dec_c_hidden, enc_hiddens)\n",
        "\n",
        "\t\ttopv, topi = pred.topk(1,dim=1)\n",
        "\t\tni = topi.view(1,-1)\n",
        "\n",
        "\t\tdecoder_input = ni\n",
        "\t\tdec_h_hidden = dec_outputs[0]\n",
        "\t\tdec_c_hidden = dec_outputs[1]\n",
        "\n",
        "\t\tloss += loss_criterion(pred,target_batch[i])\n",
        "\n",
        "\treturn loss.item() / target_batch.shape[0]"
      ],
      "metadata": {
        "id": "fwRfOdvWxfvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Computes the loss value over all of the test_batches'''\n",
        "\n",
        "def test(test_batches, encoder, decoder, loss_criterion):\n",
        "\n",
        "\twith torch.no_grad():\n",
        "\t\ttest_loss = 0\n",
        "\n",
        "\t\tfor batch in test_batches:\n",
        "\t\t\t(input_batch, target_batch) = pad_batch(batch)\n",
        "\t\t\tbatch_loss = test_batch(input_batch, target_batch, encoder, decoder, loss_criterion)\n",
        "\t\t\ttest_loss += batch_loss\n",
        "\n",
        "\treturn test_loss / len(test_batches)"
      ],
      "metadata": {
        "id": "-Hxda_8vxgkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Returns the predicted translation of a given input sentence. Predicted\n",
        "translation is trimmed to length of cutoff_length argument'''\n",
        "\n",
        "def evaluate(encoder, decoder, sentence, cutoff_length):\n",
        "\twith torch.no_grad():\n",
        "\t\tinput_variable = tensorFromSentence(input_lang, sentence)\n",
        "\t\tinput_variable = input_variable.view(-1,1)\n",
        "\t\tenc_h_hidden, enc_c_hidden = encoder.create_init_hiddens(1)\n",
        "\n",
        "\t\tenc_hiddens, enc_outputs = encoder(input_variable, enc_h_hidden, enc_c_hidden)\n",
        "\n",
        "\t\tdecoder_input = Variable(torch.LongTensor(1,1).fill_(output_lang.word_to_index.get(\"SOS\")).cuda()) if use_cuda \\\n",
        "\t\t\t\t\t\telse Variable(torch.LongTensor(1,1).fill_(output_lang.word_to_index.get(\"SOS\")))\n",
        "\t\tdec_h_hidden = enc_outputs[0]\n",
        "\t\tdec_c_hidden = enc_outputs[1]\n",
        "\n",
        "\t\tdecoded_words = []\n",
        "\n",
        "\t\tfor di in range(cutoff_length):\n",
        "\t\t\tpred, dec_outputs = decoder(decoder_input, dec_h_hidden, dec_c_hidden, enc_hiddens)\n",
        "\n",
        "\t\t\ttopv, topi = pred.topk(1,dim=1)\n",
        "\t\t\tni = topi.item()\n",
        "\t\t\tif ni == output_lang.word_to_index.get(\"EOS\"):\n",
        "\t\t\t\tdecoded_words.append('<EOS>')\n",
        "\t\t\t\tbreak\n",
        "\t\t\telse:\n",
        "\t\t\t\tdecoded_words.append(output_lang.index_to_word[ni])\n",
        "\n",
        "\t\t\tdecoder_input = Variable(torch.LongTensor(1,1).fill_(ni).cuda()) if use_cuda \\\n",
        "\t\t\t\t\t\t\telse Variable(torch.LongTensor(1,1).fill_(ni))\n",
        "\t\t\tdec_h_hidden = dec_outputs[0]\n",
        "\t\t\tdec_c_hidden = dec_outputs[1]\n",
        "\n",
        "\t\toutput_sentence = ' '.join(decoded_words)\n",
        "\n",
        "\t\treturn output_sentence"
      ],
      "metadata": {
        "id": "E7dSrdWzxlgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rcgv-j-0xmsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Evaluates prediction translations for a specified number (n) of sentences\n",
        "chosen randomly from a list of passed sentence pairs. Returns three sentences\n",
        "in the format:\n",
        "                  > input sentence\n",
        "                  = correct translation\n",
        "                  < predicted translation'''\n",
        "\n",
        "def evaluate_randomly(encoder, decoder, pairs, n=2, trim=100):\n",
        "\tfor i in range(n):\n",
        "\t\tpair = random.choice(pairs)\n",
        "\t\tprint('>', pair[0])\n",
        "\t\tprint('=', pair[1])\n",
        "\t\toutput_sentence = evaluate(encoder, decoder, pair[0],cutoff_length=trim)\n",
        "\t\tprint('<', output_sentence)\n",
        "\t\tprint('')\n",
        "\t\tif create_txt:\n",
        "\t\t\tf = open(print_to, 'a')\n",
        "\t\t\tf.write(\"\\n \\\n",
        "\t\t\t\t> %s \\n \\\n",
        "\t\t\t\t= %s \\n \\\n",
        "\t\t\t\t< %s \\n\" % (pair[0], pair[1], output_sentence))\n",
        "\t\t\tf.close()"
      ],
      "metadata": {
        "id": "F0lMB8oyx8fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Used to plot the progress of training. Plots the loss value vs. time'''\n",
        "def showPlot(times, losses, fig_name):\n",
        "    x_axis_label = 'Minutes'\n",
        "    colors = ('red','blue')\n",
        "    if max(times) >= 120:\n",
        "    \ttimes = [mins/60 for mins in times]\n",
        "    \tx_axis_label = 'Hours'\n",
        "    i = 0\n",
        "    for key, losses in losses.items():\n",
        "    \tif len(losses) > 0:\n",
        "    \t\tplt.plot(times, losses, label=key, color=colors[i])\n",
        "    \t\ti += 1\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.xlabel(x_axis_label)\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Results')\n",
        "    plt.savefig(fig_name+'.png')\n",
        "    plt.close('all')\n",
        "\n",
        "'''prints the current memory consumption'''\n",
        "def mem():\n",
        "\tif use_cuda:\n",
        "\t\tmem = torch.cuda.memory_allocated()/1e7\n",
        "\telse:\n",
        "\t\tmem = psutil.cpu_percent()\n",
        "\tprint('Current mem usage:')\n",
        "\tprint(mem)\n",
        "\treturn \"Current mem usage: %s \\n\" % (mem)\n",
        "\n",
        "'''converts a time measurement in seconds to hours'''\n",
        "def asHours(s):\n",
        "\tm = math.floor(s / 60)\n",
        "\th = math.floor(m / 60)\n",
        "\ts -= m * 60\n",
        "\tm -= h * 60\n",
        "\treturn '%dh %dm %ds' % (h, m, s)"
      ],
      "metadata": {
        "id": "JCA3M1A5x98T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''The master function that trains the model. Evlautes progress on the train set\n",
        "(if present) and also records the progress of training in both a txt file and\n",
        "a png graph. Also can save the weights of both the Encoder and Decoder\n",
        "for future use.'''\n",
        "\n",
        "def train_and_test(epochs, test_eval_every, plot_every, learning_rate,\n",
        "                   lr_schedule, train_pairs, test_pairs, input_lang,\n",
        "                   output_lang, batch_size, test_batch_size, encoder, decoder,\n",
        "                   loss_criterion, trim, save_weights):\n",
        "\n",
        "\ttimes = []\n",
        "\tlosses = {'train set':[], 'test set': []}\n",
        "\n",
        "\ttest_batches, longest_seq, n_o_b = batchify(test_pairs, input_lang,\n",
        "                                              output_lang, test_batch_size,\n",
        "                                              shuffle_data=False)\n",
        "\n",
        "\tstart = time.time()\n",
        "\tfor i in range(1,epochs+1):\n",
        "\n",
        "\t\t'''adjust the learning rate according to the learning rate schedule\n",
        "\t\tspecified in lr_schedule'''\n",
        "\t\tif i in lr_schedule.keys():\n",
        "\t\t\tlearning_rate /= lr_schedule.get(i)\n",
        "\n",
        "\n",
        "\t\tencoder.train()\n",
        "\t\tdecoder.train()\n",
        "\n",
        "\t\tencoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "\t\tdecoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "\t\tbatches, longest_seq, n_o_b = batchify(train_pairs, input_lang,\n",
        "                                           output_lang, batch_size,\n",
        "                                           shuffle_data=True)\n",
        "\t\ttrain_loss = train(batches, encoder, decoder, encoder_optimizer,\n",
        "                       decoder_optimizer, loss_criterion)\n",
        "\n",
        "\t\tnow = time.time()\n",
        "\t\tprint(\"Iter: %s \\nLearning Rate: %s \\nTime: %s \\nTrain Loss: %s \\n\"\n",
        "          % (i, learning_rate, asHours(now-start), train_loss))\n",
        "\n",
        "\t\tif create_txt:\n",
        "\t\t\twith open(print_to, 'a') as f:\n",
        "\t\t\t\tf.write(\"Iter: %s \\nLeaning Rate: %s \\nTime: %s \\nTrain Loss: %s \\n\" \\\n",
        "\t\t\t\t\t% (i, learning_rate, asHours(now-start), train_loss))\n",
        "\n",
        "\t\tif i % test_eval_every == 0:\n",
        "\t\t\tif test_pairs:\n",
        "\t\t\t\ttest_loss = test(test_batches, encoder, decoder, criterion)\n",
        "\t\t\t\tprint(\"Test set loss: %s\" % (test_loss))\n",
        "\t\t\t\tif create_txt:\n",
        "\t\t\t\t\twith open(print_to, 'a') as f:\n",
        "\t\t\t\t\t\tf.write(\"Test Loss: %s \\n\" % (test_loss))\n",
        "\t\t\t\tevaluate_randomly(encoder, decoder, test_pairs, trim)\n",
        "\t\t\telse:\n",
        "\t\t\t\tevaluate_randomly(encoder, decoder, train_pairs, trim)\n",
        "\n",
        "\t\tif i % plot_every == 0:\n",
        "\t\t\ttimes.append((time.time()-start)/60)\n",
        "\t\t\tlosses['train set'].append(train_loss)\n",
        "\t\t\tif test_pairs:\n",
        "\t\t\t\tlosses['test set'].append(test_loss)\n",
        "\t\t\tshowPlot(times, losses, output_file_name)\n",
        "\t\t\tif save_weights:\n",
        "\t\t\t\ttorch.save(encoder.state_dict(), output_file_name+'_enc_weights.pt')\n",
        "\t\t\t\ttorch.save(decoder.state_dict(), output_file_name+'_dec_weights.pt')\n"
      ],
      "metadata": {
        "id": "P9ud9ElTx9b5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"PROVIDE INFORMATION ON THE DATASET AND DATASET CLEANING\"\"\"\n",
        "\n",
        "input_lang_name = 'english'\n",
        "output_lang_name = 'sanskrit'\n",
        "\n",
        "\"\"\"name of your dataset\"\"\"\n",
        "dataset = 'SkrToEng'\n",
        "\n",
        "\"\"\"file path of dataset in the form of a tuple. If translated sentences are\n",
        "stored in two files, this tuple will have two elements\"\"\"\n",
        "raw_data_file_path = ('/content/drive/MyDrive/itihasa-main/itihasa-main/data/traineng.txt', '/content/drive/MyDrive/itihasa-main/itihasa-main/data/trainsn.txt')\n",
        "\n",
        "\n",
        "\"\"\"True if you want to reverse the order of the sentence pairs. For example,\n",
        "in our dataset the sentence pairs list the English sentence first followed by\n",
        "the French translation. But we want to translate from French to English,\n",
        "so we set reverse as True.\"\"\"\n",
        "reverse=True\n",
        "\n",
        "\"\"\"Remove sentences from dataset that are longer than trim (in either language)\"\"\"\n",
        "trim = 50\n",
        "\n",
        "\"\"\"max number of words in the vocabulary for both languages\"\"\"\n",
        "max_vocab_size= 20000\n",
        "\n",
        "\"\"\"if true removes sentences from the dataset that don't start with eng_prefixes.\n",
        "Typically will want to use False, but implemented to compare results with Pytorch\n",
        "tutorial. Can also change the eng_prefixes to prefixes of other languages or\n",
        "other English prefixes. Just be sure that the prefixes apply to the OUTPUT\n",
        "language (i.e. the language that the model is translating to NOT from)\"\"\"\n",
        "start_filter = False\n",
        "\n",
        "\"\"\"denotes what percentage of the data to use as training data. the remaining\n",
        "percentage becomes test data. Typically want to use 0.8-0.9. 1.0 used here to\n",
        "compare with PyTorch results where no test set was utilized\"\"\"\n",
        "perc_train_set = 1.0"
      ],
      "metadata": {
        "id": "qCF0DVUNyB9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"OUTPUT OPTIONS\"\"\"\n",
        "\n",
        "\"\"\"denotes how often to evaluate a loss on the test set and print\n",
        "sample predictions on the test set.\n",
        "if no test set, simply prints sample predictions on the train set.\"\"\"\n",
        "test_eval_every = 1\n",
        "\n",
        "\"\"\"denotes how often to plot the loss values of train and test (if applicable)\"\"\"\n",
        "plot_every = 1\n",
        "\n",
        "\"\"\"if true creates a txt file of the output\"\"\"\n",
        "create_txt = True\n",
        "\n",
        "\"\"\"if true saves the encoder and decoder weights to seperate .pt files for later use\"\"\"\n",
        "save_weights = False"
      ],
      "metadata": {
        "id": "pyKslkZzYlVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"HYPERPARAMETERS: FEEL FREE TO PLAY WITH THESE TO TRY TO ACHIEVE BETTER RESULTS\"\"\"\n",
        "\n",
        "\"\"\"signifies whether the Encoder and Decoder should be bidirectional LSTMs or not\"\"\"\n",
        "bidirectional = True\n",
        "if bidirectional:\n",
        "\tdirections = 2\n",
        "else:\n",
        "\tdirections = 1\n",
        "\n",
        "\"\"\"number of layers in both the Encoder and Decoder\"\"\"\n",
        "layers = 2\n",
        "\n",
        "\"\"\"Hidden size of the Encoder and Decoder\"\"\"\n",
        "hidden_size = 440\n",
        "\n",
        "\"\"\"Dropout value for Encoder and Decoder\"\"\"\n",
        "dropout = 0.2\n",
        "\n",
        "\"\"\"Training set batch size\"\"\"\n",
        "batch_size = 32\n",
        "\n",
        "\"\"\"Test set batch size\"\"\"\n",
        "test_batch_size = 32\n",
        "\n",
        "\"\"\"number of epochs (full passes through the training data)\"\"\"\n",
        "epochs = 100\n",
        "\n",
        "\"\"\"Initial learning rate\"\"\"\n",
        "learning_rate= 1\n",
        "\n",
        "\"\"\"Learning rate schedule. Signifies by what factor to divide the learning rate\n",
        "at a certain epoch. For example {5:10} would divide the learning rate by 10\n",
        "before the 5th epoch and {5:10, 10:100} would divide the learning rate by 10\n",
        "before the 5th epoch and then again by 100 before the 10th epoch\"\"\"\n",
        "lr_schedule = {}\n",
        "\n",
        "\"\"\"loss criterion, see https://pytorch.org/docs/stable/nn.html for other options\"\"\"\n",
        "criterion = nn.NLLLoss()"
      ],
      "metadata": {
        "id": "E6SDHim5alsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"******************************************************************\n",
        "********************NO NEED TO ALTER ANYTHING BELOW******************\n",
        "******************************************************************\"\"\"\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "\n",
        "\"\"\"for plotting of the loss\"\"\"\n",
        "plt.switch_backend('agg')\n",
        "\n",
        "output_file_name = \"testdata.%s_trim.%s_vocab.%s_directions.%s_layers.%s_hidden.%s_dropout.%s_learningrate.%s_batch.%s_epochs.%s\" % (dataset,trim,max_vocab_size,directions,layers,hidden_size,dropout,learning_rate,batch_size,epochs)\n",
        "\n",
        "if create_txt:\n",
        "\tprint_to = output_file_name+'.txt'\n",
        "\twith open(print_to, 'w+') as f:\n",
        "\t\tf.write(\"Starting Training \\n\")\n",
        "else:\n",
        "\tprint_to = None\n",
        "\n",
        "input_lang, output_lang, train_pairs, test_pairs = prepareData(\n",
        "    input_lang_name, output_lang_name, raw_data_file_path,\n",
        "    max_vocab_size=max_vocab_size, reverse=reverse, trim=trim,\n",
        "    start_filter=start_filter, perc_train_set=perc_train_set, print_to=print_to)\n",
        "print('Train Pairs #')\n",
        "print(len(train_pairs))\n",
        "\n",
        "\n",
        "\"\"\"for gradient clipping from\n",
        "https://github.com/pytorch/examples/blob/master/word_language_model/main.py\"\"\"\n",
        "parser = argparse.ArgumentParser(description='PyTorch Wikitext-2 RNN/LSTM Language Model')\n",
        "parser.add_argument('--clip', type=float, default=0.25,\n",
        "                    help='gradient clipping')\n",
        "args = parser.parse_args()\n",
        "\n",
        "mem()\n",
        "\n",
        "if create_txt:\n",
        "\twith open(print_to, 'a') as f:\n",
        "\t\tf.write(\"\\nRandom Train Pair: %s \\n\\nRandom Test Pair: %s \\n\\n\"\n",
        "            % (random.choice(train_pairs),random.choice(test_pairs)\n",
        "               if test_pairs else \"None\"))\n",
        "\t\tf.write(mem())\n",
        "\n",
        "\n",
        "\"\"\"create the Encoder\"\"\"\n",
        "encoder = EncoderRNN(input_lang.vocab_size, hidden_size, layers=layers,\n",
        "                     dropout=dropout, bidirectional=bidirectional)\n",
        "\n",
        "\"\"\"create the Decoder\"\"\"\n",
        "decoder = DecoderAttn(hidden_size, output_lang.vocab_size, layers=layers,\n",
        "                      dropout=dropout, bidirectional=bidirectional)\n",
        "\n",
        "print('Encoder and Decoder Created')\n",
        "mem()\n",
        "\n",
        "if use_cuda:\n",
        "\tprint('Cuda being used')\n",
        "\tencoder = encoder.cuda()\n",
        "\tdecoder = decoder.cuda()\n",
        "\n",
        "print('Number of epochs: '+str(epochs))\n",
        "\n",
        "if create_txt:\n",
        "\twith open(print_to, 'a') as f:\n",
        "\t\tf.write('Encoder and Decoder Created\\n')\n",
        "\t\tf.write(mem())\n",
        "\t\tf.write(\"Number of epochs %s \\n\" % (epochs))\n",
        "\n",
        "train_and_test(epochs, test_eval_every, plot_every, learning_rate, lr_schedule,\n",
        "               train_pairs, test_pairs, input_lang, output_lang, batch_size,\n",
        "               test_batch_size, encoder, decoder, criterion, trim, save_weights)\n",
        "\n",
        "# Save encoder model to Google Drive\n",
        "torch.save(encoder.state_dict(), '/content/drive/My Drive/encoder_model.pth')\n",
        "\n",
        "# Save decoder model to Google Drive\n",
        "torch.save(decoder.state_dict(), '/content/drive/My Drive/decoder_model.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STtSolKPaocN",
        "outputId": "902962ac-fa62-4ad9-99b2-f83612dcd75d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 75161 sentence pairs\n",
            "Trimmed to 67782 sentence pairs\n",
            "Counting words...\n",
            "Train pairs: 67782\n",
            "Test pairs: 0\n",
            "Counted Words -> Trimmed Vocabulary Sizes (w/ EOS and SOS tags):\n",
            "sanskrit, 14 -> 17\n",
            "english, 27375 -> 16379\n",
            "\n",
            "Train Pairs #\n",
            "67782\n",
            "Current mem usage:\n",
            "0.0\n",
            "Current mem usage:\n",
            "0.0\n",
            "Encoder and Decoder Created\n",
            "Current mem usage:\n",
            "0.0\n",
            "Cuda being used\n",
            "Number of epochs: 100\n",
            "Current mem usage:\n",
            "12.948224\n",
            "Iter: 1 \n",
            "Learning Rate: 1 \n",
            "Time: 0h 7m 1s \n",
            "Train Loss: 3.917825769993518 \n",
            "\n",
            "> \n",
            "= in a short time that city set on fire by the monkeys looked like the earth glowing at the time of the universal dissolution .\n",
            "< the king of the son of the king of the son of the son of the son of the son of the king of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the king of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= when my husband sleeps in peace i never awake him even if important business wanted his attention . i was happy to sit by him lying asleep .\n",
            "< the king of the son of the king of the son of the king of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= golden ar <UNK> bespattered with blood looked like masses of clouds illumined with flashes of lightning .\n",
            "< the king of the son of the king of the son of the king of the son of the son of the son of the king of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= the mighty warriors of the kurus and ourselves have performed wonderful feats of valour in the battle field . the hostile troops from the north the west the east and the south have been slain to a man .\n",
            "< the king of the son of the son of the son of the king of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= i remember from my youth that a slaughter ground for kine and the <UNK> for wine mark out the gates of the palaces of balhika kings .\n",
            "< the king of the son of the son of the king of the son of the king of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= from our good fortune our strength is increased and from our good fortune purochana is dead . o greatly effulgent one from our good fortune my great grief is also removed .\n",
            "< the king of the son of the son of the king of the son of the king of the son of the son of the son of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= o grandfathers my mind has been diverted from brahmacharya i shall certainly do your favourite work .\n",
            "< the king of the son of the son of the son of the son of the king of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= then with joyful hearts all the principal monkeys stood encircling the high souled hanuman .\n",
            "< the king of the son of the king of the son of the son of the king of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> !\n",
            "= arjuna said he depending upon whose prowess the kauravas having addressed themselves to the accomplishment of <UNK> deeds are blowing their conches and facing us with firmness .\n",
            "< the king of the son of the son of the son of the king of the son of the king of the son of the son of the king of the son of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= was will this most excellent beauty of the world this slender waisted lady having beautiful teeth and large eyes accept me as her lord ?\n",
            "< the king of the son of the son of the son of the king of the son of the son of the son of the king of the son of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= the pandavas going round many brahmanas who daily adored their fires and bending their heads unto them proceeded on their journey .\n",
            "< the king of the son of the son of the king of the son of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= the two panchala chiefs yudhamanyu and uttamanjas who were the guards of the wheels of the car of arjuna did not forsake him in battle as they were supported by the diademdecked one arjuna . \n",
            "< the king of the son of the son of the king of the son of the king of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= bhishma said this adhyatma o son of pritha that you ask me about i will presently describe . it is o son highly agreeable and productive of great happiness .\n",
            "< the king of the son of the son of the king of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= i had requested you o my lord many times before in this house to take me to the forest with you for enjoyment and you <UNK> pleased to agree .\n",
            "< the king of the son of the king of the son of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= the rishis then having as they liked plucked a number of lotuses and taken up a number of lotusstalks came up from the lake filled with joy .\n",
            "< the king of the son of the son of the king of the son of the son of the king of the son of the son of the son of the son of the son of the king of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= draupadi said o king grant this boon that bhimasena arjuna and the twins with their bows and cars be freed from slavery and gain their liberty .\n",
            "< the king of the son of the son of the son of the king of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= as they remained thus disguised they were discovered by vibhisana . thereat taking them captive he unfolded the fact to rama saying .\n",
            "< the king of the son of the son of the son of the son of the king of the son of the king of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= his soul purified of every <UNK> and making his journeys on a celestial car of great beauty he lives there o king acting like the celestial rishis and the royal sages .\n",
            "< the king of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= so long as that king does not depart from my city bestowed your grace o great yaksha and guhyaka .\n",
            "< the king of the son of the son of the king of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= the brahmana said by good luck i have come here and by good luck i have met with you . such <UNK> of religion are difficult to get in this world .\n",
            "< the king of the son of the son of the king of the son of the son of the king of the son of the son of the son of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= i shall for my part completely minister to my brother and father and shall without doubt increase my fame .\n",
            "< the king of the son of the son of the son of the king of the son of the son of the son of the king of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= after carefully pondering over all things one should relate to the king what is both agreeable and profitable but one must say what is beneficial in preference to what is merely agreeable .\n",
            "< the king of the son of the king of the son of the king of the son of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= danda became king of that picturesque valley . and having reared a fine city there he named it as <UNK> and appointed <UNK> of firm views as his priest .\n",
            "< the king of the son of the son of the son of the king of the son of the son of the king of the son of the king of the son of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= and being displeased he did nothing to stop the dispute but overlooked the fatal game and other horrible unjust deeds that were the result of it .\n",
            "< the king of the son of the son of the son of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= these and many other lords of cities have been united together for the sake of the pandavas headed by vasudeva .\n",
            "< the king of the son of the king of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= no person can see either heaven or hell . the sacred books however are the eyes of the virtuous . o king regulate your conduct according to the scriptures .\n",
            "< the king of the son of the king of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= there is a king born in the dynasty of the moon who is my friend and we shall approach him for he has great wealth in this world .\n",
            "< the king of the son of the king of the son of the king of the son of the king of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= then shraddha parva describing the funeral rites for the killed kurus then charvaka vadha who was a rakshasa but appeared as a brahmana .\n",
            "< the king of the son of the son of the son of the son of the king of the son of the king of the son of the son of the son of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= it describes the great merit of brahmanas and kine and it <UNK> the duties in relation to time and place .\n",
            "< the king of the son of the king of the son of the king of the son of the son of the son of the king of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= having thus spent a fourth part of his life in the study of the vedas and observance of vows and fasts and given the preceptor his fee the disciple should according to the ordinance <UNK> and return home for becoming a house holder .\n",
            "< the king of the son of the son of the king of the son of the son of the king of the son of the son of the son of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= for all this i do not see any fault in bhima ! do not yield to anger o slayer of pralamba ! our relationship with the pandavas is based on birth blood and love .\n",
            "< the king of the son of the son of the king of the son of the king of the son of the son of the son of the son of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= the illustrious god of fire becomes pleased with such a man . as another reward he never becomes divested of cattle and he is sure to win victory in battle .\n",
            "< the king of the son of the king of the son of the king of the son of the son of the king of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= if after subjugating the kurus o you sons of pritha and defeating all you destroy those who despise you then the succeeding portion of your life will be equal to death since what is life after killing all your all your cousins ?\n",
            "< the king of the son of the king of the son of the son of the son of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= addressed in these words by the king chyavana of bhrigu s race filled with great joy said to kushika these words in reply .\n",
            "< the king of the son of the son of the king of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= then in the great battle that took place the royal son of dhritarashtra was killed by bhimasena displayed his great prowess in the presence of many kings .\n",
            "< the king of the son of the son of the king of the son of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= no gift should be made to a brahmana who has no knowledge of the vedas gifts should be made to him only who is well read in the vedas . an improper gift and an improper acceptance beget bad consequences to both the giver and the acceptor .\n",
            "< the king of the son of the son of the king of the son of the son of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= this one is a mighty warrior and knows to fight according to diverse methods in battle and o king he will fight with your enemies casting fear aside .\n",
            "< the king of the son of the son of the king of the son of the king of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= that forest full of lions leopards rurus tigers buffaloes bear and deer swarming with various kinds of birds inhabited by robbers and low <UNK> tribes .\n",
            "< the king of the son of the son of the son of the son of the king of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= who indeed is he o master that was even now adored by your illustrious self with bows and with humility and who was eulogised in so high terms ? we desire to hear about him .\n",
            "< the king of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= then pacified by the celestials that best of reciters the brahmarsi vasistha made friends with visvamitra saying \n",
            "< the king of the son of the son of the king of the son of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= you kanka shall be my friend your vehicle shall be the same as mine you shall have plenty of clothes and <UNK> sorts of drinks and dishes . you shall look into both ins and outs of my affair i shall always keep my doors open for you .\n",
            "< the king of the son of the son of the son of the king of the son of the son of the king of the son of the son of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= rishis of great merit were his courtiers . there is nothing on earth which is unknown to him .\n",
            "< the king of the son of the son of the king of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the king of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= as a man casting off worn out clothes put on other new ones so embodied . self casting off old bodies enter into other new ones .\n",
            "< the king of the son of the son of the king of the son of the king of the son of the son of the son of the son of the son of the king of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= when such leaders of our race as bhisma drona and kripa all learned in morality and artha are present how could such a calamity at all happen ?\n",
            "< the king of the son of the son of the son of the son of the king of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= the wise person should be assured by present services . the person who is desirous of acquiring prosperity should join hands swear use sweet words adore by bending down his head and shed tears .\n",
            "< the king of the son of the king of the son of the son of the son of the king of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= as that crow fed upon the refuses of the food of the vaishya children did not care his equals and superiors so do you o karna fed by dhritarashtra s sons with the leavings of their food disregard your equals and superiors .\n",
            "< the king of the son of the son of the king of the son of the king of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= one should then go to mahakala with regulated diet and vows . having bathed in the <UNK> tirtha he obtains the fruit of a horsesacrifice .\n",
            "< the king of the son of the son of the king of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= o placid one hearing from hanuman s lips that sita has been slain by indrajit . raghava has been overcome with stupor .\n",
            "< the king of the son of the son of the king of the king of the son of the king of the son of the king of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= then in the fourth year after his exile king nala regained his wife and had all his desires satisfied and thus once more he enjoyed the highest pleasure .\n",
            "< the king of the son of the son of the son of the king of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n",
            "> \n",
            "= by committing evil acts a powerless men can never escape . men fear his conduct even as they are alarmed on seeing a wolf .\n",
            "< the king of the son of the son of the son of the king of the son of the son of the son of the king and the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the king of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript\n",
        "\n",
        "# This code will keep your Colab session active by\n",
        "# automatically scrolling the output cell every 30 minutes\n",
        "# Change the interval as needed (in milliseconds)\n",
        "interval = 60 * 60 * 1000  # 30 minutes\n",
        "Javascript(f\"setInterval(() => {{ window.scrollBy(0, 1); }}, {interval});\")\n"
      ],
      "metadata": {
        "id": "KyxRg9Kc0CTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outside_sent = \"going home\"\n",
        "outside_sent = normalizeString(outside_sent)\n",
        "evaluate(encoder, decoder, outside_sent, cutoff_length=10)"
      ],
      "metadata": {
        "id": "Jq5lmcfAQ5Cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XGvfXAeM0QI2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}